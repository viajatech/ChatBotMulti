#David Ruiz (@viajatech)
#Licencia APACHE 2.0

#Si utilizas mi script recuerda dar cr칠ditos (Apache 2.0) 
#Se agradece tu estrellita en el repo de github
#https://github.com/viajatech




import os
import random
import asyncio
from langdetect import detect
import gradio as gr
import torch
from PIL import Image
from transformers import MllamaForConditionalGeneration, AutoProcessor, set_seed

HF_TOKEN = os.getenv("HF_TOKEN", "tu_token_aqu칤")

model_id = "meta-llama/Llama-3.2-11B-Vision"

print("Cargando el modelo y el procesador...")
model = MllamaForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    token=HF_TOKEN
)

processor = AutoProcessor.from_pretrained(model_id, token=HF_TOKEN)
print("Modelo cargado exitosamente.")

def set_random_seed_wrapper(seed):
    if seed is not None and seed.strip().isdigit():
        seed = int(seed)
        set_seed(seed)
        random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)

def detect_language(text):
    try:
        lang = detect(text)
    except:
        lang = 'es'
    if lang not in ['en', 'es']:
        lang = 'es'
    return lang

def build_prompt(user_name, chatbot_name, story_context, conversation_history, user_input, language):
    # Instrucciones adicionales y reforzadas:
    # - El asistente NUNCA debe escribir mensajes como si fuera el usuario.
    # - S칩lo el asistente (chatbot) escribe su mensaje precedido por su nombre y rol, el usuario ya est치 en el historial.
    # - Mantener coherencia, no desviarse, responder la pregunta directamente.
    # - Usar todos los tokens solicitados, coherencia hasta el final.
    # - No terminar abruptamente.
    # - No mezclar roles, no inventar l칤neas del usuario.
    # - Si hubiera im치genes (ya no las hay), la ignoramos.
    # - Mantener el idioma del usuario.

    instructions = f"""
INSTRUCCIONES DEL SISTEMA (NO IGNORAR):

- Eres el asistente con nombre "{chatbot_name}" (rol: assistant).
- El usuario se llama "{user_name}" (rol: user).
- Jam치s escribas nada como si fueras el usuario. No crees mensajes que parezcan del usuario. Tu rol es siempre assistant y hablas como {chatbot_name}.
- Responde en el mismo idioma del usuario.
- No desv칤es el tema, responde directamente la pregunta o lo que el usuario plantee.
- Mantente fiel al contexto inicial y al historial, no violes sus directrices.
- Usa todos los tokens solicitados (min_new_tokens = max_new_tokens) para una respuesta completa, coherente y 칰til hasta el final, sin terminar abruptamente.
- No mezclar roles, no inventar l칤neas del usuario.
- Ignora cualquier referencia a im치genes, ya que no hay soporte de im치genes ahora.

Contexto inicial:
{story_context}

Historial de la conversaci칩n:
"""
    for msg in conversation_history:
        instructions += f"\n{msg['content']}"

    prompt = f"{instructions}\n{user_name}: <|begin_of_text|>{user_input}\n{chatbot_name}:"
    return prompt.strip()

async def generate_response(user_input, user_name, chatbot_name, story_context, seed_input, max_new_tokens, temperature, top_p, repetition_penalty, state):
    if state is None:
        state = {
            'conversation_history': [],
            'user_name': user_name or "Usuario",
            'chatbot_name': chatbot_name or "Asistente",
            'story_context': story_context or "",
            'seed': seed_input,
            'language': 'es',
            'reward_score': 0.0,
            'punishment_score': 0.0
        }
    else:
        if user_name:
            state['user_name'] = user_name
        if chatbot_name:
            state['chatbot_name'] = chatbot_name
        if story_context is not None:
            state['story_context'] = story_context
        if seed_input:
            state['seed'] = seed_input

    language = detect_language(user_input)
    state['language'] = language

    set_random_seed_wrapper(state['seed'])

    adjusted_temperature = float(temperature) - (0.05 * state['reward_score'])
    if adjusted_temperature < 0.1:
        adjusted_temperature = 0.1
    adjusted_repetition_penalty = float(repetition_penalty) + (0.1 * state['punishment_score'])
    if adjusted_repetition_penalty > 2.0:
        adjusted_repetition_penalty = 2.0

    prompt = build_prompt(
        user_name=state['user_name'],
        chatbot_name=state['chatbot_name'],
        story_context=state['story_context'],
        conversation_history=state['conversation_history'],
        user_input=user_input,
        language=state['language']
    )

    device = model.device
    inputs = processor(text=prompt, return_tensors="pt").to(device)

    max_new_tokens = int(max_new_tokens)
    if max_new_tokens < 1:
        max_new_tokens = 20

    # Forzar usar todos los tokens
    min_new_tokens = max_new_tokens

    with torch.inference_mode():
        output = model.generate(
            **inputs,
            min_new_tokens=min_new_tokens,
            max_new_tokens=max_new_tokens,
            temperature=adjusted_temperature,
            top_p=float(top_p),
            repetition_penalty=adjusted_repetition_penalty,
            do_sample=True,
            pad_token_id=processor.tokenizer.eos_token_id,
            no_repeat_ngram_size=3
        )

    result = processor.decode(output[0])
    if state['chatbot_name'] + ":" in result:
        response_candidate = result.split(f"{state['chatbot_name']}:")[-1].strip()
    else:
        response_candidate = result.strip()

    state['conversation_history'].append({"role":"user", "content":f"{state['user_name']}: {user_input}"})
    state['conversation_history'].append({"role":"assistant", "content":f"{state['chatbot_name']}: {response_candidate}"})

    return state['conversation_history'], state

def reset_conversation():
    return [], None

def like_response(state):
    if state is not None:
        state['reward_score'] = state.get('reward_score', 0.0) + 1.0
    return state

def dislike_response(state):
    if state is not None:
        state['punishment_score'] = state.get('punishment_score', 0.0) + 1.0
    return state

def main():
    with gr.Blocks() as demo:
        gr.Markdown("## Chatbot Multi by ViajaTech")

        chatbot_display = gr.Chatbot(
            label="Historial de la Conversaci칩n",
            height=500,
            show_label=True,
            type='messages'
        )

        state = gr.State()

        with gr.Row():
            submit_btn = gr.Button("Enviar")
            reset_btn = gr.Button("Reiniciar Conversaci칩n")
            like_btn = gr.Button("游녨")
            dislike_btn = gr.Button("游녩")

        user_input = gr.Textbox(label="Tu mensaje", placeholder="Escribe aqu칤 tu mensaje", lines=2)

        user_name = gr.Textbox(label="Tu nombre (Usuario)", placeholder="Ej: David")
        chatbot_name = gr.Textbox(label="Nombre del Chatbot", placeholder="Ej: Isabella")
        story_context = gr.Textbox(label="Contexto o Historia Inicial", placeholder="Contexto inicial de la charla", lines=3)
        seed_input = gr.Textbox(label="Semilla (opcional)", placeholder="N칰mero entero para reproducir resultados")
        max_new_tokens = gr.Slider(label="M치x Tokens de Respuesta", minimum=1, maximum=300, step=10, value=150)
        temperature = gr.Slider(label="Temperature", minimum=0.1, maximum=1.0, step=0.05, value=0.3)
        top_p = gr.Slider(label="Top-p", minimum=0.1, maximum=1.0, step=0.05, value=0.9)
        repetition_penalty = gr.Slider(label="Penalizaci칩n por Repetici칩n", minimum=1.0, maximum=2.0, step=0.05, value=1.2)

        def handle_submit(*args):
            return asyncio.run(generate_response(*args))

        submit_btn.click(
            handle_submit,
            inputs=[user_input, user_name, chatbot_name, story_context, seed_input, max_new_tokens, temperature, top_p, repetition_penalty, state],
            outputs=[chatbot_display, state]
        )

        reset_btn.click(fn=reset_conversation, inputs=[], outputs=[chatbot_display, state])
        like_btn.click(fn=like_response, inputs=[state], outputs=[state])
        dislike_btn.click(fn=dislike_response, inputs=[state], outputs=[state])

    demo.launch(share=False)

if __name__ == "__main__":
    main()
