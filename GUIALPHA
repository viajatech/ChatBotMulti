#David Ruiz (@viajatech)
#Licencia APACHE 2.0

#Si utilizas mi script recuerda dar cr칠ditos (Apache 2.0) 
#Se agradece tu estrellita en el repo de github
#https://github.com/viajatech




import os
import random
import asyncio
from langdetect import detect
import gradio as gr
import torch
from PIL import Image
from transformers import MllamaForConditionalGeneration, AutoProcessor, set_seed
from datetime import datetime
from dateutil import tz
import requests
from geopy.geocoders import Nominatim  # geopy para geocodificaci칩n con OSM

HF_TOKEN = os.getenv("HF_TOKEN", "tu_token_aqu칤")

model_id = "meta-llama/Llama-3.2-11B-Vision"

print("Cargando el modelo y el procesador...")
model = MllamaForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    token=HF_TOKEN
)

processor = AutoProcessor.from_pretrained(model_id, token=HF_TOKEN)
print("Modelo cargado exitosamente.")

geolocator = Nominatim(user_agent="my_geocoder")

def set_random_seed_wrapper(seed):
    if seed is not None and seed.strip().isdigit():
        seed = int(seed)
        set_seed(seed)
        random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)

def detect_language(text):
    try:
        lang = detect(text)
    except:
        lang = 'es'
    if lang not in ['en', 'es']:
        lang = 'es'
    return lang

def build_prompt(user_name, chatbot_name, story_context, conversation_history, user_input, language, internet_enabled):
    instructions = f"""
INSTRUCCIONES DEL SISTEMA (NO IGNORAR):

- Eres el asistente "{chatbot_name}" (rol: assistant).
- El usuario se llama "{user_name}" (rol: user).
- Jam치s escribas nada como si fueras el usuario.
- Responde en el mismo idioma del usuario.
- No desv칤es el tema, responde directamente la pregunta o lo que el usuario plantee.
- Mantente fiel al contexto inicial y al historial.
- Usa todos los tokens solicitados (min_new_tokens = max_new_tokens) para una respuesta completa, coherente y 칰til.
- No mezclar roles, no inventar l칤neas del usuario.
- Si '{internet_enabled}' es True, puedes obtener la hora actual y buscar lugares cercanos usando OpenStreetMap a trav칠s de Overpass.
- No terminar abruptamente.
- Si el usuario pide hora/fecha, proporci칩nala.
- Si el usuario pide lugares cercanos, b칰scalos con Overpass si internet_enabled es True.

Contexto inicial:
{story_context}

Historial de la conversaci칩n:
"""
    for msg in conversation_history:
        instructions += f"\n{msg['content']}"

    prompt = f"{instructions}\n{user_name}: <|begin_of_text|>{user_input}\n{chatbot_name}:"
    return prompt.strip()

def get_current_time():
    now = datetime.now(tz=tz.gettz('UTC')).astimezone(tz=None)
    return now.strftime("%Y-%m-%d %H:%M:%S %Z")

def search_places_osm(address, query="cafe"):
    # Geocodificar la direcci칩n con Nominatim (OpenStreetMap)
    location = geolocator.geocode(address)
    if not location:
        return "No se encontr칩 la direcci칩n."
    lat, lon = location.latitude, location.longitude

    # Usamos Overpass API para buscar lugares cercanos
    # Query de ejemplo para buscar caf칠s cerca de las coordenadas
    overpass_url = "http://overpass-api.de/api/interpreter"
    # Radio en metros
    radius = 1000
    overpass_query = f"""
    [out:json];
    node
      ["amenity"="{query}"]
      (around:{radius},{lat},{lon});
    out;
    """
    response = requests.get(overpass_url, params={'data': overpass_query})
    if response.status_code != 200:
        return "No se pudo acceder a la API de Overpass."
    data = response.json()
    if 'elements' not in data or not data['elements']:
        return f"No se encontraron lugares cercanos para '{query}'."
    # Tomamos hasta 3 resultados
    results = []
    for elem in data['elements'][:3]:
        name = elem['tags'].get('name', 'Desconocido')
        # No hay horario directamente en OSM usualmente, se necesitar칤a m치s an치lisis
        # Suponiendo que no tenemos horario:
        results.append(f"{name} - Coordenadas: {elem['lat']}, {elem['lon']}")
    return "\n".join(results)

async def generate_response(user_input, user_name, chatbot_name, story_context, seed_input, max_new_tokens, temperature, top_p, repetition_penalty, state, internet_toggle):
    if state is None:
        state = {
            'conversation_history': [],
            'user_name': user_name or "Usuario",
            'chatbot_name': chatbot_name or "Asistente",
            'story_context': story_context or "",
            'seed': seed_input,
            'language': 'es',
            'reward_score': 0.0,
            'punishment_score': 0.0
        }
    else:
        if user_name:
            state['user_name'] = user_name
        if chatbot_name:
            state['chatbot_name'] = chatbot_name
        if story_context is not None:
            state['story_context'] = story_context
        if seed_input:
            state['seed'] = seed_input

    language = detect_language(user_input)
    state['language'] = language

    set_random_seed_wrapper(state['seed'])

    adjusted_temperature = float(temperature)
    if adjusted_temperature < 0.1:
        adjusted_temperature = 0.1
    adjusted_repetition_penalty = float(repetition_penalty)

    max_new_tokens = int(max_new_tokens)
    if max_new_tokens < 1:
        max_new_tokens = 20
    min_new_tokens = max_new_tokens

    internet_enabled = (internet_toggle == True)

    additional_info = ""
    user_input_lower = user_input.lower()
    if internet_enabled:
        if "hora" in user_input_lower or "fecha" in user_input_lower:
            current_time = get_current_time()
            additional_info += f"La hora actual es: {current_time}\n"
        if "cercana" in user_input_lower or "cercano" in user_input_lower:
            # Buscamos lugares cercanos con Overpass
            # Ejemplo: suponer "milan #4 de izcalli piramide"
            # y que el usuario busca una "cafeteria"
            # Esto es un ejemplo muy simplificado.
            address = "milan #4 de izcalli piramide"
            results = search_places_osm(address, query="cafe")
            additional_info += f"Lugares cercanos:\n{results}\n"

    prompt = build_prompt(
        user_name=state['user_name'],
        chatbot_name=state['chatbot_name'],
        story_context=state['story_context'],
        conversation_history=state['conversation_history'],
        user_input=user_input + "\n" + additional_info,
        language=state['language'],
        internet_enabled=internet_enabled
    )

    device = model.device
    inputs = processor(text=prompt, return_tensors="pt").to(device)

    with torch.inference_mode():
        output = model.generate(
            **inputs,
            min_new_tokens=min_new_tokens,
            max_new_tokens=max_new_tokens,
            temperature=adjusted_temperature,
            top_p=float(top_p),
            repetition_penalty=adjusted_repetition_penalty,
            do_sample=True,
            pad_token_id=processor.tokenizer.eos_token_id,
            no_repeat_ngram_size=3
        )

    result = processor.decode(output[0])
    if state['chatbot_name'] + ":" in result:
        response_candidate = result.split(f"{state['chatbot_name']}:")[-1].strip()
    else:
        response_candidate = result.strip()

    state['conversation_history'].append({"role":"user", "content":f"{state['user_name']}: {user_input}"})
    state['conversation_history'].append({"role":"assistant", "content":f"{state['chatbot_name']}: {response_candidate}"})

    return state['conversation_history'], state

def reset_conversation():
    return [], None

def like_response(state):
    if state is not None:
        state['reward_score'] = state.get('reward_score', 0.0) + 1.0
    return state

def dislike_response(state):
    if state is not None:
        state['punishment_score'] = state.get('punishment_score', 0.0) + 1.0
    return state

def main():
    with gr.Blocks() as demo:
        gr.Markdown("## Chatbot Multi by ViajaTech")

        chatbot_display = gr.Chatbot(
            label="Historial de la Conversaci칩n",
            height=500,
            show_label=True,
            type='messages'
        )

        state = gr.State()

        with gr.Row():
            submit_btn = gr.Button("Enviar")
            reset_btn = gr.Button("Reiniciar Conversaci칩n")
            like_btn = gr.Button("游녨")
            dislike_btn = gr.Button("游녩")
            internet_toggle = gr.Checkbox(label="Buscar en Internet", value=False)

        user_input = gr.Textbox(label="Tu mensaje", placeholder="Escribe aqu칤 tu mensaje", lines=2)

        user_name = gr.Textbox(label="Tu nombre (Usuario)", placeholder="Ej: David")
        chatbot_name = gr.Textbox(label="Nombre del Chatbot", placeholder="Ej: Isabella")
        story_context = gr.Textbox(label="Contexto o Historia Inicial", placeholder="Contexto inicial de la charla", lines=3)
        seed_input = gr.Textbox(label="Semilla (opcional)", placeholder="N칰mero entero para reproducir resultados")
        max_new_tokens = gr.Slider(label="M치x Tokens de Respuesta", minimum=1, maximum=300, step=10, value=150)
        temperature = gr.Slider(label="Temperature", minimum=0.1, maximum=1.0, step=0.05, value=0.3)
        top_p = gr.Slider(label="Top-p", minimum=0.1, maximum=1.0, step=0.05, value=0.9)
        repetition_penalty = gr.Slider(label="Penalizaci칩n por Repetici칩n", minimum=1.0, maximum=2.0, step=0.05, value=1.2)

        def handle_submit(*args):
            return asyncio.run(generate_response(*args))

        submit_btn.click(
            handle_submit,
            inputs=[user_input, user_name, chatbot_name, story_context, seed_input, max_new_tokens, temperature, top_p, repetition_penalty, state, internet_toggle],
            outputs=[chatbot_display, state]
        )

        reset_btn.click(fn=reset_conversation, inputs=[], outputs=[chatbot_display, state])
        like_btn.click(fn=like_response, inputs=[state], outputs=[state])
        dislike_btn.click(fn=dislike_response, inputs=[state], outputs=[state])

    demo.launch(share=False)

if __name__ == "__main__":
    main()
